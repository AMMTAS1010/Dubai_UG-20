{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7f7874-51e1-4444-8390-e20bce5d4285",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Delivery and Restaurant Datasets\n",
    "\n",
    "This notebook demonstrates the preprocessing steps for tabular datasets related to delivery operations and restaurants. \n",
    "The tasks include:\n",
    "- Handling missing values\n",
    "- Feature selection\n",
    "- Balancing class distributions\n",
    "- Splitting data into training, validation, and test sets\n",
    "\n",
    "The processed datasets will be saved for further analysis and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9db1b-34dc-4ab8-90ef-763dfa43b172",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "The required libraries for data preprocessing include pandas, numpy, and machine learning preprocessing tools like SMOTENC and LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98cff972-1169-40b9-b8ec-817ad945447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0fc26-e197-4b01-8bf8-8c49bfeccb04",
   "metadata": {},
   "source": [
    "## Define Preprocessing Function\n",
    "\n",
    "The `preprocess_tabular_data` function handles:\n",
    "1. Loading the dataset\n",
    "2. Filling missing values\n",
    "3. Processing dataset-specific features\n",
    "4. Encoding categorical variables\n",
    "5. Balancing class distributions with SMOTENC\n",
    "6. Splitting data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003ea94-098b-4c71-a148-5fcc05a5c5bf",
   "metadata": {},
   "source": [
    "## Define File Check and Load Data\n",
    "\n",
    "The `preprocess_tabular_data` function begins by checking if the dataset file exists and then loading it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef97d3f3-b305-42b0-bf40-e3b125b648aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tabular_data(file_path, dataset_type, output_dir):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The dataset file {file_path} does not exist. Please check the path.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036c255-befc-43b5-91ca-95bd487e39f6",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "Missing values in numerical columns are filled with the median, and categorical columns are filled with \"Missing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0670ae-5f15-4758-a349-1cd29c5e1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data, dataset_type):\n",
    "    if dataset_type == \"delivery\":\n",
    "        numerical_columns = [\"Delivery_person_Age\", \"Delivery_person_Ratings\", \"multiple_deliveries\"]\n",
    "        categorical_columns = [\"City\", \"Weather_conditions\", \"Festival\"]\n",
    "    elif dataset_type == \"restaurant\":\n",
    "        numerical_columns = [\"rate (out of 5)\", \"avg cost (two people)\"]\n",
    "        categorical_columns = [\"online_order\", \"table booking\", \"restaurant type\", \"cuisines type\", \"area\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset type selected.\")\n",
    "\n",
    "    # Fill missing numerical values with median\n",
    "    for col in numerical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "    # Fill missing categorical values with \"Missing\"\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(\"Missing\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa64c42-1ef5-451c-ad19-4b1c636ff9a9",
   "metadata": {},
   "source": [
    "## Dataset-Specific Feature Processing\n",
    "\n",
    "Each dataset (delivery or restaurant) has unique features and a target column that need specific handling:\n",
    "1. Delivery data: Target column is binned into low, medium, and high categories.\n",
    "2. Restaurant data: Rare classes are combined for balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab47a10d-1c33-4b33-83d8-f8df05f784c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_specific_features(data, dataset_type):\n",
    "    if dataset_type == \"delivery\":\n",
    "        selected_features = [\n",
    "            \"Time_taken (min)\",\n",
    "            \"Delivery_person_Age\",\n",
    "            # \"Delivery_person_Ratings\",  # Removed to prevent data leakage\n",
    "            \"Restaurant_latitude\",\n",
    "            \"Restaurant_longitude\",\n",
    "            \"Delivery_location_latitude\",\n",
    "            \"Delivery_location_longitude\",\n",
    "            \"Vehicle_condition\",\n",
    "            \"multiple_deliveries\",\n",
    "            \"Weather_conditions\",\n",
    "            \"City\",\n",
    "            \"Festival\",\n",
    "        ]\n",
    "        target_column = \"Delivery_person_Ratings\"\n",
    "\n",
    "        # Convert target column to categorical bins\n",
    "        bins = [0, 2.5, 4.0, data[target_column].max()]\n",
    "        labels = [\"Low\", \"Medium\", \"High\"]\n",
    "        data[target_column] = pd.cut(\n",
    "            data[target_column],\n",
    "            bins=bins,\n",
    "            labels=labels,\n",
    "            include_lowest=True,\n",
    "            right=False\n",
    "        )\n",
    "\n",
    "        # Check for NaNs in target column after binning\n",
    "        nan_count = data[target_column].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"Warning: {nan_count} NaNs found in target column after binning. Dropping these rows.\")\n",
    "            data = data.dropna(subset=[target_column])\n",
    "\n",
    "        # Display class distribution\n",
    "        print(\"Class distribution after binning:\")\n",
    "        print(data[target_column].value_counts())\n",
    "\n",
    "    elif dataset_type == \"restaurant\":\n",
    "        selected_features = [\n",
    "            \"num of ratings\",\n",
    "            \"avg cost (two people)\",\n",
    "            \"online_order\",\n",
    "            \"table booking\",\n",
    "            \"restaurant type\",\n",
    "            \"cuisines type\",\n",
    "            \"area\",\n",
    "        ]\n",
    "        target_column = \"rate (out of 5)\"\n",
    "\n",
    "        # Convert target column to categorical bins\n",
    "        bins = [0, 1, 2, 3, 4, data[target_column].max()]\n",
    "        labels = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "        data[target_column] = pd.cut(\n",
    "            data[target_column],\n",
    "            bins=bins,\n",
    "            labels=labels,\n",
    "            include_lowest=True,\n",
    "            right=False\n",
    "        )\n",
    "\n",
    "        # Check for NaNs in target column after binning\n",
    "        nan_count = data[target_column].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"Warning: {nan_count} NaNs found in target column after binning. Dropping these rows.\")\n",
    "            data = data.dropna(subset=[target_column])\n",
    "\n",
    "        # **Combine rare classes with too few samples**\n",
    "        min_samples_required = 5  # You can adjust this threshold\n",
    "        class_counts = data[target_column].value_counts()\n",
    "        rare_classes = class_counts[class_counts < min_samples_required].index.tolist()\n",
    "\n",
    "        if rare_classes:\n",
    "            print(f\"Combining rare classes {rare_classes} with class '3'\")\n",
    "            data[target_column] = data[target_column].replace(rare_classes, '3')\n",
    "\n",
    "        # Display class distribution after addressing rare classes\n",
    "        print(\"Class distribution after addressing rare classes:\")\n",
    "        print(data[target_column].value_counts())\n",
    "\n",
    "    # Filter dataset to include only selected features and target column\n",
    "    available_features = [feature for feature in selected_features if feature in data.columns]\n",
    "    X = data[available_features]\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Drop rows with remaining missing values in X and y\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    combined = combined.dropna()\n",
    "    X = combined[available_features]\n",
    "    y = combined[target_column]\n",
    "    return data, selected_features, target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a624a-8042-43dd-a946-8643fd438dea",
   "metadata": {},
   "source": [
    "## Class Balancing with SMOTENC\n",
    "\n",
    "To address class imbalance, the SMOTENC algorithm is used to generate synthetic samples for minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68183546-0eb1-45e6-ab61-e5c296b08aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(data, X, y, selected_features):\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_features_indices = [X.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "    # Encode target variable for SMOTENC using LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Recompute class counts after addressing rare classes\n",
    "    class_counts = pd.Series(y_encoded).value_counts()\n",
    "    min_class_size = class_counts.min()\n",
    "    print(\"Class counts after preprocessing:\")\n",
    "    print(class_counts)\n",
    "\n",
    "    # Adjust k_neighbors based on min class size\n",
    "    k_neighbors = min(min_class_size - 1, 5)\n",
    "    if k_neighbors < 1:\n",
    "        k_neighbors = 1  # Ensure k_neighbors is at least 1\n",
    "    print(f\"Using k_neighbors = {k_neighbors} for SMOTENC.\")\n",
    "\n",
    "    # Balance dataset using SMOTENC with adjusted k_neighbors\n",
    "    smotenc = SMOTENC(\n",
    "        categorical_features=categorical_features_indices,\n",
    "        random_state=42,\n",
    "        k_neighbors=k_neighbors,\n",
    "    )\n",
    "    X_resampled, y_resampled = smotenc.fit_resample(X, y_encoded)\n",
    "\n",
    "    # Decode target variable back to original categories\n",
    "    y_resampled = le.inverse_transform(y_resampled)\n",
    "\n",
    "    # Check for NaNs in X_resampled and y_resampled\n",
    "    if X_resampled.isna().any().any():\n",
    "        print(\"Warning: NaNs found in X_resampled after SMOTENC.\")\n",
    "        X_resampled = X_resampled.dropna()\n",
    "        y_resampled = y_resampled[X_resampled.index]\n",
    "\n",
    "    if pd.isna(y_resampled).any():\n",
    "        print(\"Warning: NaNs found in y_resampled after SMOTENC.\")\n",
    "        X_resampled = X_resampled[~pd.isna(y_resampled)]\n",
    "        y_resampled = y_resampled[~pd.isna(y_resampled)]\n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c740e54-57a9-46ca-9dde-f61a66f23e8d",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split\n",
    "\n",
    "The preprocessed and balanced data is split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afeddf7c-49e8-48e8-bf4b-c9b28017822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06ab96-7482-4e61-b8b8-77752a504116",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data\n",
    "\n",
    "The processed datasets are saved as CSV files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b346e0f4-6842-4ab6-a63f-997353617748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(X_train, X_valid, X_test, y_train, y_valid, y_test, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the processed datasets to CSV files\n",
    "    X_train.to_csv(os.path.join(output_dir, \"X_train.csv\"), index=False)\n",
    "    pd.DataFrame(y_train, columns=[\"target\"]).to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False)\n",
    "\n",
    "    X_valid.to_csv(os.path.join(output_dir, \"X_valid.csv\"), index=False)\n",
    "    pd.DataFrame(y_valid, columns=[\"target\"]).to_csv(os.path.join(output_dir, \"y_valid.csv\"), index=False)\n",
    "\n",
    "    X_test.to_csv(os.path.join(output_dir, \"X_test.csv\"), index=False)\n",
    "    pd.DataFrame(y_test, columns=[\"target\"]).to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False)\n",
    "\n",
    "    print(f\"Processed datasets saved to {output_dir}\")\n",
    "\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392ca62-b75e-4b00-a136-8f966b236b62",
   "metadata": {},
   "source": [
    "## Main Function for Dataset Selection\n",
    "\n",
    "This function allows users to select the dataset type and initiates the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f399e84b-d7ad-4906-8536-bf059fbf39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Select Dataset Type:\")\n",
    "    print(\"1. Delivery Dataset\")\n",
    "    print(\"2. Restaurant Dataset\")\n",
    "    choice = input(\"Enter your choice (1/2): \").strip()\n",
    "\n",
    "    # Get the base directory for datasets\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'Datasets'))\n",
    "    # Define the base output directory for preprocessed data\n",
    "    base_output_dir = os.path.join(base_dir, 'preprocessed_data')\n",
    "\n",
    "    if choice == \"1\":\n",
    "        dataset_type = \"delivery\"\n",
    "        file_path = os.path.join(base_dir, \"Original_Datasets\", \"Zomato Dataset.csv\")\n",
    "        output_dir = os.path.join(base_output_dir, \"delivery\")\n",
    "    elif choice == \"2\":\n",
    "        dataset_type = \"restaurant\"\n",
    "        file_path = os.path.join(base_dir,  \"Original_Datasets\", \"zomato.csv\")\n",
    "        output_dir = os.path.join(base_output_dir, \"restaurant\")\n",
    "    else:\n",
    "        print(\"Invalid choice. Please select 1 or 2\")\n",
    "        return\n",
    "\n",
    "    data = preprocess_tabular_data(file_path, dataset_type, output_dir)\n",
    "    data = handle_missing_values(data, dataset_type)\n",
    "    data, selected_features, target_column = process_specific_features(data, dataset_type)\n",
    "    X = data[selected_features]\n",
    "    y = data[target_column]\n",
    "\n",
    "    X_resampled, y_resampled = balance_classes(data, X, y, selected_features)\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = split_data(X_resampled, y_resampled)\n",
    "    save_data(X_train, X_valid, X_test, y_train, y_valid, y_test, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce60e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Dataset Type:\n",
      "1. Delivery Dataset\n",
      "2. Restaurant Dataset\n",
      "Warning: 53 NaNs found in target column after binning. Dropping these rows.\n",
      "Class distribution after binning:\n",
      "Delivery_person_Ratings\n",
      "High      44164\n",
      "Medium     1329\n",
      "Low          38\n",
      "Name: count, dtype: int64\n",
      "Class counts after preprocessing:\n",
      "0    44164\n",
      "2     1329\n",
      "1       38\n",
      "Name: count, dtype: int64\n",
      "Using k_neighbors = 5 for SMOTENC.\n",
      "Processed datasets saved to /Users/abdallahalshaqra/Desktop/DMML/Dubai_UG-20/CW_progress/Coursework_Code_Files/Datasets/preprocessed_data/delivery\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
